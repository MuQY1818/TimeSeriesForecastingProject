"""
æ•°æ®å·¥å…·æ¨¡å—

æä¾›æ•°æ®å¤„ç†å’Œè½¬æ¢çš„å·¥å…·å‡½æ•°ã€‚
"""

import numpy as np
import pandas as pd
import torch
from typing import Tuple, List
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.api import acf

def analyze_dataset_characteristics(df: pd.DataFrame, target_col: str):
    """
    å¯¹æ—¶é—´åºåˆ—æ•°æ®é›†è¿›è¡Œå…¨é¢çš„è‡ªåŠ¨åŒ–åˆ†æã€‚
    """
    print("\n" + "="*40)
    print("ğŸ”¬ æ­£åœ¨æ‰§è¡Œè‡ªåŠ¨åŒ–æ•°æ®é›†åˆ†æ...")
    print("="*40)
    
    results = {}
    
    # 1. åŸºæœ¬ä¿¡æ¯
    results['time_range_start'] = df['date'].min().strftime('%Y-%m-%d')
    results['time_range_end'] = df['date'].max().strftime('%Y-%m-%d')
    results['duration_days'] = (df['date'].max() - df['date'].min()).days
    results['total_records'] = len(df)
    
    # 2. ç¼ºå¤±å€¼åˆ†æ
    results['missing_values_count'] = int(df[target_col].isnull().sum())
    results['missing_values_percentage'] = float(df[target_col].isnull().mean() * 100)
    
    # 3. ç›®æ ‡å˜é‡ç»Ÿè®¡
    target_series = df[target_col].dropna()
    results['target_mean'] = float(target_series.mean())
    results['target_std'] = float(target_series.std())
    results['target_min'] = float(target_series.min())
    results['target_max'] = float(target_series.max())
    results['target_skewness'] = float(target_series.skew())
    results['target_kurtosis'] = float(target_series.kurt())
    
    # 4. å¹³ç¨³æ€§æ£€éªŒ (ADF Test)
    try:
        adf_result = adfuller(target_series)
        results['adf_statistic'] = float(adf_result[0])
        results['adf_p_value'] = float(adf_result[1])
        results['is_stationary'] = bool(adf_result[1] < 0.05)
    except Exception as e:
        results['adf_test_error'] = str(e)
        results['is_stationary'] = 'Unknown'
        
    # 5. å­£èŠ‚æ€§åˆ†æ (ACF)
    try:
        # è®¡ç®—ACFï¼Œnlagså¯ä»¥è®¾ç½®ä¸ºæ€»æ•°æ®ç‚¹çš„ä¸€åŠï¼Œæˆ–è€…ä¸€ä¸ªå›ºå®šçš„è¾ƒå¤§å€¼
        autocorr = acf(target_series, nlags=min(365*2, len(target_series)//2 - 1), fft=True)
        # å¯»æ‰¾é™¤äº†lag 0ä¹‹å¤–çš„ç¬¬ä¸€ä¸ªæ˜¾è‘—å³°å€¼
        significant_peaks = np.where(autocorr > 2 / np.sqrt(len(target_series)))[0]
        peak_lags = significant_peaks[significant_peaks > 0]
        
        if len(peak_lags) > 0:
            # å¸¸è§çš„å­£èŠ‚æ€§å‘¨æœŸ
            common_periods = [7, 30, 90, 365]
            detected_seasonality = []
            for p in common_periods:
                # æ£€æŸ¥åœ¨å‘¨æœŸpé™„è¿‘æ˜¯å¦å­˜åœ¨å³°å€¼ï¼ˆä¾‹å¦‚ï¼Œp-2 åˆ° p+2 çš„èŒƒå›´å†…ï¼‰
                if any(abs(lag - p) <= 2 for lag in peak_lags):
                    detected_seasonality.append(p)
            results['detected_seasonality_periods'] = detected_seasonality
        else:
            results['detected_seasonality_periods'] = []
    except Exception as e:
        results['seasonality_analysis_error'] = str(e)

    # æ‰“å°æ€»ç»“
    print("ğŸ“‹ æ•°æ®é›†åˆ†ææŠ¥å‘Š:")
    for key, value in results.items():
        print(f"  - {key}: {value}")
    print("="*40 + "\n")
    
    return results

def create_sequences(data: np.ndarray, seq_length: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
    
    Args:
        data: è¾“å…¥æ•°æ®
        seq_length: åºåˆ—é•¿åº¦
        
    Returns:
        X: è¾“å…¥åºåˆ—
        y: ç›®æ ‡å€¼
    """
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:(i + seq_length)])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

def prepare_data_for_model(data: pd.DataFrame, target_col: str, seq_length: int, 
                          train_size: float = 0.8) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    å‡†å¤‡æ¨¡å‹è®­ç»ƒæ•°æ®
    
    Args:
        data: è¾“å…¥æ•°æ®æ¡†
        target_col: ç›®æ ‡åˆ—å
        seq_length: åºåˆ—é•¿åº¦
        train_size: è®­ç»ƒé›†æ¯”ä¾‹
        
    Returns:
        X_train: è®­ç»ƒé›†è¾“å…¥
        y_train: è®­ç»ƒé›†ç›®æ ‡
        X_test: æµ‹è¯•é›†è¾“å…¥
        y_test: æµ‹è¯•é›†ç›®æ ‡
    """
    # æå–ç›®æ ‡åˆ—
    target_data = data[target_col].values
    
    # åˆ›å»ºåºåˆ—
    X, y = create_sequences(target_data, seq_length)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_size = int(len(X) * train_size)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_train = torch.FloatTensor(X_train)
    y_train = torch.FloatTensor(y_train)
    X_test = torch.FloatTensor(X_test)
    y_test = torch.FloatTensor(y_test)
    
    return X_train, y_train, X_test, y_test

def get_time_series_data(dataset_type='min_daily_temps'):
    """
    ä»CSVæ–‡ä»¶åŠ è½½å¹¶å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®ã€‚
    - å°†åˆ—åé‡å‘½åä¸º 'date' å’Œ 'temp'ã€‚
    - å°† 'date' åˆ—è½¬æ¢ä¸ºdatetimeå¯¹è±¡ã€‚
    - æŒ‰æ—¥æœŸæ’åºã€‚
    """
    if dataset_type == 'total_cleaned':
        csv_path = 'data/total_cleaned.csv'
        df = pd.read_csv(csv_path)
        # å‡è®¾åˆ—åä¸º 'æ—¥æœŸ' å’Œ 'æˆäº¤å•†å“ä»¶æ•°'
        df.rename(columns={'æ—¥æœŸ': 'date', 'æˆäº¤å•†å“ä»¶æ•°': 'temp'}, inplace=True)
    else: # é»˜è®¤ä¸º min_daily_temps
        csv_path = 'data/min_daily_temps.csv'
        df = pd.read_csv(csv_path)
        df.rename(columns={'Date': 'date', 'Temp': 'temp'}, inplace=True)
        
    df['date'] = pd.to_datetime(df['date'])
    df.sort_values('date', inplace=True)
    df.reset_index(drop=True, inplace=True)
    print(f"âœ… æ•°æ®åŠ è½½è‡ª: {csv_path}, Shape: {df.shape}")
    return df 