"""
Sonataå®éªŒæ¨¡å— - ä½¿ç”¨é‡æ„åçš„TLAFSæ¡†æ¶
"""
import os
from datetime import datetime
import json

# ä»é‡æ„åçš„tlafsæ¡†æ¶ä¸­å¯¼å…¥å¿…è¦çš„æ¨¡å—
from tlafs.core.algorithm import TLAFS_Algorithm
from tlafs.utils.data_utils import get_time_series_data
from tlafs.features.generator import execute_plan
from tlafs.utils.evaluation import probe_feature_set, evaluate_on_multiple_models
from tlafs.visualization.plotting import visualize_final_predictions
from tlafs.utils.file_utils import save_results # å‡è®¾è¿™ä¸ªå‡½æ•°ä»ç„¶é€‚ç”¨
from tlafs.analysis.feature_importance import calculate_permutation_importance, analyze_iterative_contribution
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.metrics import r2_score
import pandas as pd

def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œå®Œæ•´çš„Sonata T-LAFSå®éªŒã€‚"""
    
    # ===== é…ç½®å˜é‡ =====
    DATASET_TYPE = 'min_daily_temps'
    N_ITERATIONS = 10
    TARGET_COL = 'temp'
    
    print("="*80)
    print(f"ğŸš€ T-LAFS Sonataå®éªŒ: ä½¿ç”¨æ¨¡å—åŒ–æ¡†æ¶")
    print("="*80)

    # --- 1. åˆå§‹åŒ–ç¯å¢ƒå’Œæ•°æ® ---
    run_timestamp = datetime.now().strftime("sonata_%Y%m%d_%H%M%S")
    results_dir = os.path.join("results", run_timestamp)
    os.makedirs(results_dir, exist_ok=True)
    print(f"ğŸ“‚ æœ¬æ¬¡è¿è¡Œçš„æ‰€æœ‰ç»“æœå°†ä¿å­˜åœ¨: {results_dir}")

    base_df = get_time_series_data(DATASET_TYPE)

    # --- 2. è¿è¡Œ T-LAFS ç‰¹å¾æœç´¢ ---
    tlafs_alg = TLAFS_Algorithm(
        base_df=base_df,
        target_col=TARGET_COL,
        n_iterations=N_ITERATIONS,
        results_dir=results_dir
    )
    
    # TLAFS_Algorithmçš„__init__ç°åœ¨ä¼šè‡ªåŠ¨è°ƒç”¨é¢„è®­ç»ƒ
    
    # å®šä¹‰TLAFSè¿è¡Œæ‰€éœ€çš„å‚æ•°å­—å…¸
    # è¿™äº›å‚æ•°ä¼šè¢«ä¼ é€’ç»™ execute_plan å‡½æ•°
    tlafs_params = {
        "target_col_static": tlafs_alg.target_col_static,
        "pretrain_cols_static": tlafs_alg.pretrain_cols_static,
        "pretrained_encoders": tlafs_alg.pretrained_encoders,
        "embedder_scalers": tlafs_alg.embedder_scalers,
        # å¦‚æœæœ‰å…¶ä»–æ¨¡å‹ï¼Œä¹Ÿåœ¨è¿™é‡Œæ·»åŠ 
        # "meta_forecast_models": tlafs_alg.meta_forecast_models,
        # "meta_scalers": tlafs_alg.meta_scalers,
    }

    # å°†æ‰§è¡Œå’Œæ¢æµ‹å‡½æ•°ä¼ é€’ç»™runæ–¹æ³•
    best_df, best_feature_plan, best_score_during_search = tlafs_alg.run(
        execute_plan_func=lambda df, plan: execute_plan(df, plan, tlafs_params),
        probe_func=probe_feature_set
    )

    # --- 3. å¯¹æ‰¾åˆ°çš„æœ€ä½³ç‰¹å¾é›†è¿›è¡Œæœ€ç»ˆåˆ†æ ---
    if best_df is not None:
        probe_name_for_reporting = "Sonata_Transformer_Specialist"
        print("\n" + "="*40)
        print(f"ğŸ”¬ å¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œæœ€ç»ˆéªŒè¯ ({probe_name_for_reporting}) ğŸ”¬")
        print("="*40)
        
        final_metrics, final_results = evaluate_on_multiple_models(
            best_df,
            TARGET_COL
        )

        if final_metrics:
            best_final_model_name = max(final_metrics, key=lambda k: final_metrics[k]['r2'])
            best_final_metrics = final_metrics[best_final_model_name]
            
            # ä»final_resultsä¸­ä¸ºæœ€ä½³æ¨¡å‹è·å–æ—¥æœŸã€çœŸå®å€¼å’Œé¢„æµ‹å€¼
            # æ³¨æ„ï¼ševaluate_on_multiple_modelsè¿”å›çš„y_testæ˜¯åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ä¸ºpd.Seriesä»¥ä¾¿ä½¿ç”¨ç´¢å¼•
            best_result_data = final_results[best_final_model_name]
            
            # æˆ‘ä»¬éœ€è¦ä»best_dfä¸­è·å–æ­£ç¡®çš„æ—¥æœŸç´¢å¼•
            # evaluate_on_multiple_modelså†…éƒ¨è¿›è¡Œäº†train_test_splitï¼Œæˆ‘ä»¬éœ€è¦è·å–æµ‹è¯•é›†çš„ç´¢å¼•
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾evaluate_on_multiple_modelsè¿”å›çš„y_trueçš„ç´¢å¼•ä¸best_dfçš„æµ‹è¯•éƒ¨åˆ†åŒ¹é…
            # ä¸€ä¸ªæ›´å¥å£®çš„æ–¹æ³•æ˜¯è®©è¯„ä¼°å‡½æ•°ä¹Ÿè¿”å›æµ‹è¯•ç´¢å¼•
            test_indices = best_df.index[-len(best_result_data['y_true']):]


            visualize_final_predictions(
                dates=best_df.loc[test_indices, 'date'],
                y_true=best_result_data['y_true'],
                y_pred=best_result_data['y_pred'],
                best_model_name=best_final_model_name,
                probe_name=probe_name_for_reporting,
                best_model_metrics=best_final_metrics,
                results_dir=results_dir
            )

            # --- 3.5. ç‰¹å¾é‡è¦æ€§åˆ†æ ---
            print("\n" + "="*40)
            print(f"ğŸ”¬ ç‰¹å¾é‡è¦æ€§åˆ†æ ğŸ”¬")
            print("="*40)

            # a) è¿­ä»£è¾¹é™…è´¡çŒ®åˆ†æ
            print("\n--- è¿­ä»£è¾¹é™…è´¡çŒ® (Iterative Marginal Contribution) ---")
            print("æ­¤åˆ†ææ˜¾ç¤ºäº†åœ¨T-LAFSæœç´¢è¿‡ç¨‹ä¸­ï¼ŒæŒ‰é¡ºåºæ·»åŠ æ¯ä¸ªç‰¹å¾æ—¶æ¨¡å‹æ€§èƒ½çš„å˜åŒ–ã€‚")
            iterative_contribution_df = analyze_iterative_contribution(tlafs_alg.history)
            if not iterative_contribution_df.empty:
                print(iterative_contribution_df.to_string())
                # ä¿å­˜åˆ°CSV
                iterative_contribution_path = os.path.join(results_dir, "iterative_contribution.csv")
                iterative_contribution_df.to_csv(iterative_contribution_path, index=False)
                print(f"\nâœ… è¿­ä»£è´¡çŒ®åˆ†æå·²ä¿å­˜è‡³: {iterative_contribution_path}")

            # b) æ’åˆ—é‡è¦æ€§åˆ†æ
            print("\n--- æ’åˆ—é‡è¦æ€§ (Permutation Importance) ---")
            print("æ­¤åˆ†ææ˜¾ç¤ºäº†åœ¨æœ€ç»ˆæ¨¡å‹ä¸­ï¼Œéšæœºæ‰“ä¹±æ¯ä¸ªç‰¹å¾åå¯¹æ¨¡å‹æ€§èƒ½ï¼ˆR^2ï¼‰é€ æˆçš„è´Ÿé¢å½±å“ã€‚")
            
            # ä¸ºäº†è¿›è¡Œæ­¤åˆ†æï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹å’ŒéªŒè¯é›†
            # æˆ‘ä»¬å°†ä½¿ç”¨LightGBMï¼Œå› ä¸ºå®ƒé€Ÿåº¦å¿«ä¸”æ€§èƒ½å¥½
            features = [col for col in best_df.columns if col not in ['date', TARGET_COL]]
            X = best_df[features]
            y = best_df[TARGET_COL]

            # ç®€å•æ‹†åˆ†æ•°æ®ç”¨äºåˆ†æ
            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

            # è®­ç»ƒä¸€ä¸ªLGBMæ¨¡å‹
            lgbm_for_importance = lgb.LGBMRegressor(random_state=42)
            lgbm_for_importance.fit(X_train, y_train)

            # è®¡ç®—æ’åˆ—é‡è¦æ€§
            permutation_importance_df = calculate_permutation_importance(
                model=lgbm_for_importance,
                X_val=X_val,
                y_val=y_val,
                metric_func=r2_score
            )
            print(permutation_importance_df.to_string())

            # ä¿å­˜åˆ°CSV
            perm_importance_path = os.path.join(results_dir, "permutation_importance.csv")
            permutation_importance_df.to_csv(perm_importance_path)
            print(f"\nâœ… æ’åˆ—é‡è¦æ€§åˆ†æå·²ä¿å­˜è‡³: {perm_importance_path}")


            # --- 4. ä¿å­˜æ‰€æœ‰ç»“æœ ---
            summary_data = {
                "probe_model": probe_name_for_reporting,
                "best_score_during_search": best_score_during_search,
                "best_feature_plan": best_feature_plan,
                "final_features": [col for col in best_df.columns if col not in ['date', TARGET_COL]],
                "final_validation_scores": final_metrics,
                "best_final_model": {
                    "name": best_final_model_name,
                    "metrics": best_final_metrics
                },
                "run_history": tlafs_alg.history
            }
            
            # ä½¿ç”¨ä¸€ä¸ªé€šç”¨çš„JSONä¿å­˜å‡½æ•°
            results_path = os.path.join(results_dir, "sonata_tlafs_summary.json")
            with open(results_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, indent=4, default=str) # ä½¿ç”¨default=strå¤„ç†Numpyç±»å‹
            print(f"âœ… æœ€ç»ˆæ€»ç»“å·²ä¿å­˜è‡³: {results_path}")

    else:
        print("\nT-LAFSæœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„ç‰¹å¾è®¡åˆ’ã€‚")

if __name__ == "__main__":
    main()