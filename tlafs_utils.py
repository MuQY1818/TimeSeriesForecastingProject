import pandas as pd
import numpy as np
import torch
from mvse_embedding import MVSEEmbedding
from mvse_probe_integration import create_mvse_probe_features, train_mvse_probe_model

def generate_mvse_features_for_tlafs(df, target_col='temp', hist_len=90, num_lags=14):
    """
    ä¸º T-LAFS æ¡†æ¶ç”Ÿæˆ MVSE æ¢é’ˆç‰¹å¾
    
    Args:
        df (pd.DataFrame): è¾“å…¥æ•°æ®
        target_col (str): ç›®æ ‡åˆ—å
        hist_len (int): å†å²åºåˆ—é•¿åº¦
        num_lags (int): æ»åç‰¹å¾æ•°é‡
        
    Returns:
        pd.DataFrame: æ·»åŠ äº† MVSE ç‰¹å¾çš„æ•°æ®æ¡†
    """
    print("ğŸ”® ç”Ÿæˆ MVSE æ¢é’ˆç‰¹å¾ç”¨äº T-LAFS...")
    
    # åˆ›å»ºç‰¹å¾
    hist_sequences, lag_features, targets, valid_indices, target_scaler = create_mvse_probe_features(
        df, target_col=target_col, hist_len=hist_len, num_lags=num_lags
    )
    
    # è®­ç»ƒæ¨¡å‹
    model, best_loss = train_mvse_probe_model(
        hist_sequences, lag_features, targets,
        epochs=50, mask_rate=0.3
    )
    
    # ç”Ÿæˆç‰¹å¾
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    
    with torch.no_grad():
        # è·å– MVSE ç¼–ç ç‰¹å¾
        hist_tensor = torch.FloatTensor(hist_sequences).to(device)
        mvse_features = model.mvse_encoder(hist_tensor).cpu().numpy()
        
        # è·å–æ± åŒ–ç‰¹å¾ç”¨äºåˆ†æ
        pooling_features = model.mvse_encoder.get_pooling_features(hist_tensor)
        gap_features = pooling_features['gap'].cpu().numpy()
        gmp_features = pooling_features['gmp'].cpu().numpy()
        masked_gap_features = pooling_features['masked_gap'].cpu().numpy()
    
    # åˆ›å»ºç‰¹å¾ DataFrame
    feature_names = []
    all_features = []
    
    # 1. MVSE ä¸»è¦ç‰¹å¾
    mvse_cols = [f"mvse_feat_{i}" for i in range(mvse_features.shape[1])]
    feature_names.extend(mvse_cols)
    all_features.append(mvse_features)
    
    # 2. æ± åŒ–ç‰¹å¾çš„ç»Ÿè®¡æ‘˜è¦
    gap_stats = np.column_stack([
        gap_features.mean(axis=1),
        gap_features.std(axis=1),
        gap_features.max(axis=1),
        gap_features.min(axis=1)
    ])
    gap_stat_cols = ['mvse_gap_mean', 'mvse_gap_std', 'mvse_gap_max', 'mvse_gap_min']
    feature_names.extend(gap_stat_cols)
    all_features.append(gap_stats)
    
    gmp_stats = np.column_stack([
        gmp_features.mean(axis=1),
        gmp_features.std(axis=1),
        gmp_features.max(axis=1),
        gmp_features.min(axis=1)
    ])
    gmp_stat_cols = ['mvse_gmp_mean', 'mvse_gmp_std', 'mvse_gmp_max', 'mvse_gmp_min']
    feature_names.extend(gmp_stat_cols)
    all_features.append(gmp_stats)
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    final_features = np.concatenate(all_features, axis=1)
    
    # åˆ›å»º DataFrame
    features_df = pd.DataFrame(
        final_features,
        index=valid_indices,
        columns=feature_names
    )
    
    print(f"âœ… MVSE æ¢é’ˆç‰¹å¾ç”Ÿæˆå®Œæˆ:")
    print(f"   - ç‰¹å¾æ•°é‡: {len(feature_names)}")
    print(f"   - æ ·æœ¬æ•°é‡: {len(features_df)}")
    print(f"   - è®­ç»ƒæŸå¤±: {best_loss:.6f}")
    
    # ä½¿ç”¨ shift(1) é¿å…æ•°æ®æ³„éœ²
    return df.join(features_df.shift(1)) 