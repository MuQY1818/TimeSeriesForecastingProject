import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class MVSEEmbedding(nn.Module):
    """
    Multi-View Sequential Embedding (MVSE) æ¨¡å—
    
    å°†æ—¶é—´åºåˆ— (B, T, D) ç¼–ç æˆå…¨å±€ä½ç»´ç‰¹å¾å‘é‡ (B, d_out)
    ä½¿ç”¨ä¸‰ç§ä¸åŒçš„æ± åŒ–ç­–ç•¥ï¼šGAPã€GMPã€MaskedGAP
    
    Args:
        d_input (int): è¾“å…¥ç‰¹å¾ç»´åº¦ D
        d_hidden (int): éšè—å±‚ç»´åº¦
        d_out (int): è¾“å‡ºç‰¹å¾ç»´åº¦
        mask_rate (float): éšæœºé®ç½©æ¯”ä¾‹ï¼ŒèŒƒå›´ [0, 1)
        dropout (float): Dropout æ¯”ä¾‹ï¼Œé»˜è®¤ 0.1
    """
    
    def __init__(self, d_input, d_hidden, d_out, mask_rate=0.3, dropout=0.1):
        super(MVSEEmbedding, self).__init__()
        
        self.d_input = d_input
        self.d_hidden = d_hidden
        self.d_out = d_out
        self.mask_rate = mask_rate
        
        # æ‹¼æ¥åçš„ç‰¹å¾ç»´åº¦ï¼š3ç§æ± åŒ– Ã— è¾“å…¥ç»´åº¦
        self.concat_dim = 3 * d_input
        
        # LayerNorm ç”¨äºå½’ä¸€åŒ–æ‹¼æ¥åçš„ç‰¹å¾
        self.layer_norm = nn.LayerNorm(self.concat_dim)
        
        # å‰é¦ˆç½‘ç»œï¼šä¸¤å±‚çº¿æ€§å±‚ + ReLU + Dropout
        self.feedforward = nn.Sequential(
            nn.Linear(self.concat_dim, d_hidden),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_hidden, d_out),
            nn.Sigmoid()  # æœ€ç»ˆä½¿ç”¨ Sigmoid æ¿€æ´»
        )
        
    def global_average_pooling(self, x):
        """
        å…¨å±€å¹³å‡æ± åŒ– (GAP)
        
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ (B, T, D)
            
        Returns:
            torch.Tensor: æ± åŒ–ç»“æœ (B, D)
        """
        # åœ¨æ—¶é—´ç»´åº¦ T ä¸Šæ±‚å¹³å‡
        return torch.mean(x, dim=1)  # (B, T, D) -> (B, D)
    
    def global_max_pooling(self, x):
        """
        å…¨å±€æœ€å¤§æ± åŒ– (GMP)
        
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ (B, T, D)
            
        Returns:
            torch.Tensor: æ± åŒ–ç»“æœ (B, D)
        """
        # åœ¨æ—¶é—´ç»´åº¦ T ä¸Šæ±‚æœ€å¤§å€¼
        return torch.max(x, dim=1)[0]  # (B, T, D) -> (B, D)ï¼Œ[0]å–å€¼ï¼Œ[1]å–ç´¢å¼•
    
    def masked_global_average_pooling(self, x):
        """
        éšæœºé®ç½©å¹³å‡æ± åŒ– (MaskedGAP)
        
        ç±»ä¼¼ Dropoutï¼Œéšæœºå°†éƒ¨åˆ†æ—¶é—´æ­¥ç½®é›¶ï¼Œç„¶åå¯¹å‰©ä½™å€¼æ±‚å¹³å‡
        
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ (B, T, D)
            
        Returns:
            torch.Tensor: æ± åŒ–ç»“æœ (B, D)
        """
        B, T, D = x.shape
        
        if self.training and self.mask_rate > 0:
            # è®­ç»ƒæ¨¡å¼ä¸‹åº”ç”¨éšæœºé®ç½©
            # ç”Ÿæˆé®ç½©ï¼š1è¡¨ç¤ºä¿ç•™ï¼Œ0è¡¨ç¤ºé®ç½©
            mask = torch.rand(B, T, 1, device=x.device) > self.mask_rate  # (B, T, 1)
            
            # åº”ç”¨é®ç½©
            masked_x = x * mask.float()  # (B, T, D)
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬å®é™…ä¿ç•™çš„æ—¶é—´æ­¥æ•°é‡
            valid_counts = mask.sum(dim=1, keepdim=True).float()  # (B, 1, 1)
            valid_counts = torch.clamp(valid_counts, min=1.0)  # é¿å…é™¤é›¶
            
            # è®¡ç®—é®ç½©åçš„å¹³å‡å€¼
            masked_sum = torch.sum(masked_x, dim=1)  # (B, D)
            masked_avg = masked_sum / valid_counts.squeeze(-1)  # (B, D)
            
            return masked_avg
        else:
            # æ¨ç†æ¨¡å¼ä¸‹æˆ–mask_rate=0æ—¶ï¼Œç›´æ¥ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–
            return self.global_average_pooling(x)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x (torch.Tensor): è¾“å…¥æ—¶é—´åºåˆ— (B, T, D)
            
        Returns:
            torch.Tensor: ç¼–ç åçš„ç‰¹å¾å‘é‡ (B, d_out)
        """
        # æ£€æŸ¥è¾“å…¥ç»´åº¦
        if len(x.shape) != 3:
            raise ValueError(f"è¾“å…¥åº”ä¸º3ç»´å¼ é‡ (B, T, D)ï¼Œä½†å¾—åˆ°å½¢çŠ¶: {x.shape}")
        
        B, T, D = x.shape
        if D != self.d_input:
            raise ValueError(f"è¾“å…¥ç‰¹å¾ç»´åº¦åº”ä¸º {self.d_input}ï¼Œä½†å¾—åˆ° {D}")
        
        # 1. åº”ç”¨ä¸‰ç§æ± åŒ–ç­–ç•¥
        gap_features = self.global_average_pooling(x)      # (B, D)
        gmp_features = self.global_max_pooling(x)          # (B, D)
        masked_gap_features = self.masked_global_average_pooling(x)  # (B, D)
        
        # 2. æ‹¼æ¥ä¸‰ç§æ± åŒ–ç»“æœ
        concat_features = torch.cat([
            gap_features,           # å…¨å±€å¹³å‡
            gmp_features,           # å…¨å±€æœ€å¤§
            masked_gap_features     # é®ç½©å¹³å‡
        ], dim=1)  # (B, 3*D)
        
        # 3. LayerNorm å½’ä¸€åŒ–
        normalized_features = self.layer_norm(concat_features)  # (B, 3*D)
        
        # 4. å‰é¦ˆç½‘ç»œé™ç»´
        output = self.feedforward(normalized_features)  # (B, d_out)
        
        return output
    
    def get_pooling_features(self, x):
        """
        è·å–ä¸‰ç§æ± åŒ–çš„ä¸­é—´ç‰¹å¾ï¼Œç”¨äºåˆ†æå’Œå¯è§†åŒ–
        
        Args:
            x (torch.Tensor): è¾“å…¥æ—¶é—´åºåˆ— (B, T, D)
            
        Returns:
            dict: åŒ…å«ä¸‰ç§æ± åŒ–ç»“æœçš„å­—å…¸
        """
        gap_features = self.global_average_pooling(x)
        gmp_features = self.global_max_pooling(x)
        masked_gap_features = self.masked_global_average_pooling(x)
        
        return {
            'gap': gap_features,
            'gmp': gmp_features,
            'masked_gap': masked_gap_features,
            'concat': torch.cat([gap_features, gmp_features, masked_gap_features], dim=1)
        }


def test_mvse_embedding():
    """
    æµ‹è¯• MVSEEmbedding æ¨¡å—çš„åŠŸèƒ½
    """
    print("ğŸ§ª æµ‹è¯• MVSEEmbedding æ¨¡å—...")
    
    # è®¾ç½®å‚æ•°
    batch_size = 4
    seq_len = 100
    d_input = 64
    d_hidden = 128
    d_out = 32
    mask_rate = 0.3
    
    # åˆ›å»ºæ¨¡å—
    mvse = MVSEEmbedding(
        d_input=d_input,
        d_hidden=d_hidden,
        d_out=d_out,
        mask_rate=mask_rate
    )
    
    print(f"ğŸ“Š æ¨¡å—å‚æ•°:")
    print(f"   - è¾“å…¥ç»´åº¦: {d_input}")
    print(f"   - éšè—ç»´åº¦: {d_hidden}")
    print(f"   - è¾“å‡ºç»´åº¦: {d_out}")
    print(f"   - é®ç½©æ¯”ä¾‹: {mask_rate}")
    print(f"   - æ€»å‚æ•°é‡: {sum(p.numel() for p in mvse.parameters()):,}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, seq_len, d_input)
    print(f"\nğŸ“¥ è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # è®­ç»ƒæ¨¡å¼æµ‹è¯•
    mvse.train()
    output_train = mvse(x)
    print(f"ğŸ“¤ è®­ç»ƒæ¨¡å¼è¾“å‡ºå½¢çŠ¶: {output_train.shape}")
    print(f"ğŸ“ˆ è®­ç»ƒæ¨¡å¼è¾“å‡ºèŒƒå›´: [{output_train.min():.4f}, {output_train.max():.4f}]")
    
    # æ¨ç†æ¨¡å¼æµ‹è¯•
    mvse.eval()
    with torch.no_grad():
        output_eval = mvse(x)
        print(f"ğŸ“¤ æ¨ç†æ¨¡å¼è¾“å‡ºå½¢çŠ¶: {output_eval.shape}")
        print(f"ğŸ“ˆ æ¨ç†æ¨¡å¼è¾“å‡ºèŒƒå›´: [{output_eval.min():.4f}, {output_eval.max():.4f}]")
    
    # æµ‹è¯•æ± åŒ–ç‰¹å¾
    with torch.no_grad():
        pooling_features = mvse.get_pooling_features(x)
        print(f"\nğŸ” æ± åŒ–ç‰¹å¾åˆ†æ:")
        for name, features in pooling_features.items():
            print(f"   - {name}: {features.shape}, èŒƒå›´: [{features.min():.4f}, {features.max():.4f}]")
    
    # æµ‹è¯•ä¸åŒé®ç½©æ¯”ä¾‹çš„å½±å“
    print(f"\nğŸ­ æµ‹è¯•ä¸åŒé®ç½©æ¯”ä¾‹çš„å½±å“:")
    mvse.train()
    for mask_rate in [0.0, 0.2, 0.5, 0.8]:
        mvse.mask_rate = mask_rate
        output = mvse(x)
        print(f"   - mask_rate={mask_rate}: è¾“å‡ºå‡å€¼={output.mean():.4f}, æ ‡å‡†å·®={output.std():.4f}")
    
    print("\nâœ… MVSEEmbedding æ¨¡å—æµ‹è¯•å®Œæˆï¼")
    
    # é¢å¤–æµ‹è¯•ï¼šéªŒè¯è®­ç»ƒå’Œæ¨ç†æ¨¡å¼çš„å·®å¼‚
    print(f"\nğŸ”„ éªŒè¯è®­ç»ƒ/æ¨ç†æ¨¡å¼å·®å¼‚:")
    mvse.mask_rate = 0.5  # è®¾ç½®è¾ƒé«˜çš„é®ç½©æ¯”ä¾‹
    
    mvse.train()
    train_outputs = []
    for _ in range(5):
        train_outputs.append(mvse(x))
    train_std = torch.stack(train_outputs).std(dim=0).mean()
    
    mvse.eval()
    with torch.no_grad():
        eval_output = mvse(x)
    
    print(f"   - è®­ç»ƒæ¨¡å¼å¤šæ¬¡è¿è¡Œçš„æ ‡å‡†å·®: {train_std:.6f} (åº”è¯¥>0ï¼Œå› ä¸ºæœ‰éšæœºé®ç½©)")
    print(f"   - æ¨ç†æ¨¡å¼è¾“å‡º: ç¡®å®šæ€§çš„ (æ— éšæœºæ€§)")


if __name__ == "__main__":
    test_mvse_embedding() 