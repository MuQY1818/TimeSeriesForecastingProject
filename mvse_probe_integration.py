import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import warnings
from mvse_embedding import MVSEEmbedding

warnings.filterwarnings('ignore')


class MVSEProbeForecaster(nn.Module):
    """
    åŸºäº MVSEEmbedding çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹
    
    ç»“åˆäº†å¤šè§†è§’åºåˆ—ç¼–ç å’Œä¼ ç»Ÿçš„æ»åç‰¹å¾
    """
    
    def __init__(self, input_dim, mvse_d_hidden=128, mvse_d_out=32, 
                 num_lags=14, mask_rate=0.3, final_hidden=64):
        super(MVSEProbeForecaster, self).__init__()
        
        self.input_dim = input_dim
        self.num_lags = num_lags
        self.mvse_d_out = mvse_d_out
        
        # MVSE ç¼–ç å™¨ï¼šå°†å†å²åºåˆ—ç¼–ç ä¸ºå…¨å±€ç‰¹å¾
        self.mvse_encoder = MVSEEmbedding(
            d_input=input_dim,
            d_hidden=mvse_d_hidden,
            d_out=mvse_d_out,
            mask_rate=mask_rate
        )
        
        # æœ€ç»ˆé¢„æµ‹å±‚ï¼šç»“åˆ MVSE ç‰¹å¾å’Œæ»åç‰¹å¾
        total_features = mvse_d_out + num_lags  # MVSEç‰¹å¾ + æ»åç‰¹å¾
        self.predictor = nn.Sequential(
            nn.Linear(total_features, final_hidden),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(final_hidden, final_hidden // 2),
            nn.ReLU(),
            nn.Linear(final_hidden // 2, 1)
        )
        
    def forward(self, hist_seq, lag_features):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            hist_seq (torch.Tensor): å†å²åºåˆ— (B, T, D)
            lag_features (torch.Tensor): æ»åç‰¹å¾ (B, num_lags)
            
        Returns:
            torch.Tensor: é¢„æµ‹ç»“æœ (B, 1)
        """
        # 1. ä½¿ç”¨ MVSE ç¼–ç å†å²åºåˆ—
        mvse_features = self.mvse_encoder(hist_seq)  # (B, mvse_d_out)
        
        # 2. æ‹¼æ¥ MVSE ç‰¹å¾å’Œæ»åç‰¹å¾
        combined_features = torch.cat([mvse_features, lag_features], dim=1)  # (B, total_features)
        
        # 3. æœ€ç»ˆé¢„æµ‹
        prediction = self.predictor(combined_features)  # (B, 1)
        
        return prediction


def create_mvse_probe_features(df, target_col='temp', hist_len=90, num_lags=14):
    """
    ä¸º MVSEProbeForecaster åˆ›å»ºç‰¹å¾
    
    Args:
        df (pd.DataFrame): è¾“å…¥æ•°æ®
        target_col (str): ç›®æ ‡åˆ—å
        hist_len (int): å†å²åºåˆ—é•¿åº¦
        num_lags (int): æ»åç‰¹å¾æ•°é‡
        
    Returns:
        tuple: (hist_sequences, lag_features, targets, valid_indices)
    """
    print(f"ğŸ”§ åˆ›å»º MVSE æ¢é’ˆç‰¹å¾...")
    print(f"   - å†å²åºåˆ—é•¿åº¦: {hist_len}")
    print(f"   - æ»åç‰¹å¾æ•°é‡: {num_lags}")
    
    # ç¡®ä¿æœ‰å¿…è¦çš„æ—¶é—´ç‰¹å¾
    df_copy = df.copy()
    if 'dayofweek' not in df_copy:
        df_copy['dayofweek'] = df_copy['date'].dt.dayofweek
    if 'month' not in df_copy:
        df_copy['month'] = df_copy['date'].dt.month
    
    # æ ‡å‡†åŒ–ç›®æ ‡å˜é‡
    target_scaler = MinMaxScaler()
    df_copy['temp_scaled'] = target_scaler.fit_transform(df_copy[[target_col]])
    
    # å‡†å¤‡å¤šç»´è¾“å…¥ç‰¹å¾ (temp, dayofweek, month)
    feature_cols = ['temp_scaled', 'dayofweek', 'month']
    
    hist_sequences = []
    lag_features = []
    targets = []
    valid_indices = []
    
    # æ»‘åŠ¨çª—å£åˆ›å»ºåºåˆ—
    for i in range(hist_len + num_lags, len(df_copy)):
        # 1. å†å²åºåˆ— (ç”¨äº MVSE ç¼–ç )
        hist_start = i - hist_len - num_lags
        hist_end = i - num_lags
        hist_seq = df_copy.iloc[hist_start:hist_end][feature_cols].values
        hist_sequences.append(hist_seq)
        
        # 2. æ»åç‰¹å¾ (æœ€è¿‘çš„ num_lags ä¸ªå€¼)
        lag_start = i - num_lags
        lag_end = i
        lag_vals = df_copy.iloc[lag_start:lag_end]['temp_scaled'].values
        lag_features.append(lag_vals)
        
        # 3. ç›®æ ‡å€¼ (å½“å‰æ—¶åˆ»)
        target_val = df_copy.iloc[i]['temp_scaled']
        targets.append(target_val)
        
        # 4. è®°å½•æœ‰æ•ˆç´¢å¼•
        valid_indices.append(df_copy.index[i])
    
    hist_sequences = np.array(hist_sequences, dtype=np.float32)
    lag_features = np.array(lag_features, dtype=np.float32)
    targets = np.array(targets, dtype=np.float32)
    
    print(f"âœ… ç‰¹å¾åˆ›å»ºå®Œæˆ:")
    print(f"   - å†å²åºåˆ—å½¢çŠ¶: {hist_sequences.shape}")
    print(f"   - æ»åç‰¹å¾å½¢çŠ¶: {lag_features.shape}")
    print(f"   - ç›®æ ‡å€¼å½¢çŠ¶: {targets.shape}")
    
    return hist_sequences, lag_features, targets, valid_indices, target_scaler


def train_mvse_probe_model(hist_sequences, lag_features, targets, 
                          input_dim=3, epochs=100, batch_size=32, 
                          learning_rate=0.001, mask_rate=0.3):
    """
    è®­ç»ƒ MVSEProbeForecaster æ¨¡å‹
    """
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ MVSEProbeForecaster...")
    
    # æ•°æ®åˆ†å‰²
    train_size = int(len(hist_sequences) * 0.8)
    
    X_hist_train = hist_sequences[:train_size]
    X_lag_train = lag_features[:train_size]
    y_train = targets[:train_size]
    
    X_hist_val = hist_sequences[train_size:]
    X_lag_val = lag_features[train_size:]
    y_val = targets[train_size:]
    
    # è½¬æ¢ä¸ºå¼ é‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    X_hist_train = torch.FloatTensor(X_hist_train).to(device)
    X_lag_train = torch.FloatTensor(X_lag_train).to(device)
    y_train = torch.FloatTensor(y_train).unsqueeze(1).to(device)
    
    X_hist_val = torch.FloatTensor(X_hist_val).to(device)
    X_lag_val = torch.FloatTensor(X_lag_val).to(device)
    y_val = torch.FloatTensor(y_val).unsqueeze(1).to(device)
    
    # åˆ›å»ºæ¨¡å‹
    model = MVSEProbeForecaster(
        input_dim=input_dim,
        mask_rate=mask_rate,
        num_lags=lag_features.shape[1]
    ).to(device)
    
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒè®¾ç½®
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    patience = 15
    patience_counter = 0
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        
        # ç®€å•çš„æ‰¹æ¬¡è®­ç»ƒ
        for i in range(0, len(X_hist_train), batch_size):
            end_idx = min(i + batch_size, len(X_hist_train))
            
            batch_hist = X_hist_train[i:end_idx]
            batch_lag = X_lag_train[i:end_idx]
            batch_y = y_train[i:end_idx]
            
            optimizer.zero_grad()
            predictions = model(batch_hist, batch_lag)
            loss = criterion(predictions, batch_y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        avg_train_loss = train_loss / (len(X_hist_train) // batch_size + 1)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        with torch.no_grad():
            val_predictions = model(X_hist_val, X_lag_val)
            val_loss = criterion(val_predictions, y_val).item()
        
        # æ—©åœæ£€æŸ¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_model_state = model.state_dict()
        else:
            patience_counter += 1
        
        if (epoch + 1) % 10 == 0:
            print(f"   Epoch {epoch+1:3d}: Train Loss={avg_train_loss:.6f}, Val Loss={val_loss:.6f}")
        
        if patience_counter >= patience:
            print(f"   æ—©åœè§¦å‘ï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_model_state)
    
    return model, best_val_loss


def generate_mvse_probe_features_for_tlafs(df, target_col='temp'):
    """
    ä¸º T-LAFS æ¡†æ¶ç”Ÿæˆ MVSE æ¢é’ˆç‰¹å¾
    
    è¿™ä¸ªå‡½æ•°å¯ä»¥é›†æˆåˆ°ç°æœ‰çš„ T-LAFS ç³»ç»Ÿä¸­
    """
    print("ğŸ”® ç”Ÿæˆ MVSE æ¢é’ˆç‰¹å¾ç”¨äº T-LAFS...")
    
    # åˆ›å»ºç‰¹å¾
    hist_sequences, lag_features, targets, valid_indices, target_scaler = create_mvse_probe_features(
        df, target_col=target_col, hist_len=90, num_lags=14
    )
    
    # è®­ç»ƒæ¨¡å‹
    model, best_loss = train_mvse_probe_model(
        hist_sequences, lag_features, targets,
        epochs=50, mask_rate=0.3
    )
    
    # ç”Ÿæˆç‰¹å¾
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    
    with torch.no_grad():
        # è·å– MVSE ç¼–ç ç‰¹å¾
        hist_tensor = torch.FloatTensor(hist_sequences).to(device)
        mvse_features = model.mvse_encoder(hist_tensor).cpu().numpy()
        
        # è·å–æ± åŒ–ç‰¹å¾ç”¨äºåˆ†æ
        pooling_features = model.mvse_encoder.get_pooling_features(hist_tensor)
        gap_features = pooling_features['gap'].cpu().numpy()
        gmp_features = pooling_features['gmp'].cpu().numpy()
        masked_gap_features = pooling_features['masked_gap'].cpu().numpy()
    
    # åˆ›å»ºç‰¹å¾ DataFrame
    feature_names = []
    all_features = []
    
    # 1. MVSE ä¸»è¦ç‰¹å¾
    mvse_cols = [f"mvse_feat_{i}" for i in range(mvse_features.shape[1])]
    feature_names.extend(mvse_cols)
    all_features.append(mvse_features)
    
    # 2. æ± åŒ–ç‰¹å¾çš„ç»Ÿè®¡æ‘˜è¦
    gap_stats = np.column_stack([
        gap_features.mean(axis=1),
        gap_features.std(axis=1),
        gap_features.max(axis=1),
        gap_features.min(axis=1)
    ])
    gap_stat_cols = ['mvse_gap_mean', 'mvse_gap_std', 'mvse_gap_max', 'mvse_gap_min']
    feature_names.extend(gap_stat_cols)
    all_features.append(gap_stats)
    
    gmp_stats = np.column_stack([
        gmp_features.mean(axis=1),
        gmp_features.std(axis=1),
        gmp_features.max(axis=1),
        gmp_features.min(axis=1)
    ])
    gmp_stat_cols = ['mvse_gmp_mean', 'mvse_gmp_std', 'mvse_gmp_max', 'mvse_gmp_min']
    feature_names.extend(gmp_stat_cols)
    all_features.append(gmp_stats)
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    final_features = np.concatenate(all_features, axis=1)
    
    # åˆ›å»º DataFrame
    features_df = pd.DataFrame(
        final_features,
        index=valid_indices,
        columns=feature_names
    )
    
    print(f"âœ… MVSE æ¢é’ˆç‰¹å¾ç”Ÿæˆå®Œæˆ:")
    print(f"   - ç‰¹å¾æ•°é‡: {len(feature_names)}")
    print(f"   - æ ·æœ¬æ•°é‡: {len(features_df)}")
    print(f"   - è®­ç»ƒæŸå¤±: {best_loss:.6f}")
    
    # ä½¿ç”¨ shift(1) é¿å…æ•°æ®æ³„éœ²
    return df.join(features_df.shift(1))


def test_mvse_integration():
    """
    æµ‹è¯• MVSE é›†æˆåŠŸèƒ½
    """
    print("ğŸ§ª æµ‹è¯• MVSE é›†æˆåŠŸèƒ½...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    dates = pd.date_range('2020-01-01', periods=1000, freq='D')
    np.random.seed(42)
    
    # ç”Ÿæˆå¸¦æœ‰å­£èŠ‚æ€§å’Œè¶‹åŠ¿çš„æ—¶é—´åºåˆ—
    t = np.arange(len(dates))
    seasonal = 10 * np.sin(2 * np.pi * t / 365.25)  # å¹´åº¦å­£èŠ‚æ€§
    trend = 0.01 * t  # çº¿æ€§è¶‹åŠ¿
    noise = np.random.normal(0, 2, len(dates))
    temp = 20 + seasonal + trend + noise
    
    df = pd.DataFrame({
        'date': dates,
        'temp': temp
    })
    
    print(f"ğŸ“Š æ¨¡æ‹Ÿæ•°æ®: {len(df)} ä¸ªæ ·æœ¬")
    
    # æµ‹è¯• MVSE ç‰¹å¾ç”Ÿæˆ
    df_with_mvse = generate_mvse_probe_features_for_tlafs(df, target_col='temp')
    
    # æ£€æŸ¥ç»“æœ
    mvse_cols = [col for col in df_with_mvse.columns if 'mvse_' in col]
    print(f"ğŸ¯ ç”Ÿæˆçš„ MVSE ç‰¹å¾: {len(mvse_cols)} ä¸ª")
    print(f"   å‰5ä¸ªç‰¹å¾: {mvse_cols[:5]}")
    
    # ç®€å•çš„æ€§èƒ½æµ‹è¯•
    from sklearn.ensemble import RandomForestRegressor
    
    # å‡†å¤‡æ•°æ®
    feature_cols = mvse_cols
    df_clean = df_with_mvse.dropna()
    
    if len(df_clean) > 100 and len(feature_cols) > 0:
        X = df_clean[feature_cols]
        y = df_clean['temp']
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False
        )
        
        # è®­ç»ƒéšæœºæ£®æ—
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X_train, y_train)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred = rf.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        
        print(f"ğŸ¯ MVSE ç‰¹å¾æ€§èƒ½æµ‹è¯•:")
        print(f"   - RÂ² å¾—åˆ†: {r2:.4f}")
        print(f"   - ç‰¹å¾é‡è¦æ€§å‰5: {sorted(zip(feature_cols, rf.feature_importances_), key=lambda x: x[1], reverse=True)[:5]}")
    
    print("\nâœ… MVSE é›†æˆæµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_mvse_integration() 